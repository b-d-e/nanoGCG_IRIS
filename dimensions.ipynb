{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57597a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeepSeek R1 model...\n",
      "Replace the model loading section with your actual loaded DeepSeek R1 model.\n",
      "Then call: print_activation_dimensions(your_model)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def print_activation_dimensions(model: nn.Module, model_name: str = \"DeepSeek R1\"):\n",
    "    \"\"\"\n",
    "    Print activation dimensions across all layers of a loaded model.\n",
    "    Useful for GCG implementation and refusal direction analysis.\n",
    "    \"\"\"\n",
    "    print(f\"=== {model_name} Layer Activation Dimensions ===\\n\")\n",
    "    \n",
    "    # Get model configuration\n",
    "    config = model.config\n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"  Hidden size: {config.hidden_size}\")\n",
    "    print(f\"  Number of layers: {config.num_hidden_layers}\")\n",
    "    print(f\"  Number of attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"  Intermediate size: {getattr(config, 'intermediate_size', 'N/A')}\")\n",
    "    print(f\"  Vocab size: {config.vocab_size}\")\n",
    "    print(f\"  Max position embeddings: {getattr(config, 'max_position_embeddings', 'N/A')}\")\n",
    "    print()\n",
    "    \n",
    "    layer_info = []\n",
    "    \n",
    "    # Iterate through all named modules\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            weight_shape = tuple(module.weight.shape)\n",
    "            \n",
    "            # Categorize layer types for GCG targeting\n",
    "            layer_type = get_layer_type(name, module)\n",
    "            \n",
    "            layer_info.append({\n",
    "                'name': name,\n",
    "                'type': layer_type,\n",
    "                'weight_shape': weight_shape,\n",
    "                'module_type': type(module).__name__\n",
    "            })\n",
    "    \n",
    "    # Print layer information grouped by type\n",
    "    print_layers_by_type(layer_info)\n",
    "    \n",
    "    # Print summary for GCG implementation\n",
    "    print_gcg_summary(layer_info, config)\n",
    "\n",
    "def get_layer_type(name: str, module: nn.Module) -> str:\n",
    "    \"\"\"Categorize layer types for GCG analysis.\"\"\"\n",
    "    name_lower = name.lower()\n",
    "    \n",
    "    if 'embed' in name_lower:\n",
    "        return 'embedding'\n",
    "    elif 'attention' in name_lower or 'attn' in name_lower:\n",
    "        if 'q_proj' in name_lower or 'query' in name_lower:\n",
    "            return 'attention_query'\n",
    "        elif 'k_proj' in name_lower or 'key' in name_lower:\n",
    "            return 'attention_key'\n",
    "        elif 'v_proj' in name_lower or 'value' in name_lower:\n",
    "            return 'attention_value'\n",
    "        elif 'o_proj' in name_lower or 'out' in name_lower:\n",
    "            return 'attention_output'\n",
    "        else:\n",
    "            return 'attention_other'\n",
    "    elif 'mlp' in name_lower or 'feed_forward' in name_lower or 'ffn' in name_lower:\n",
    "        if 'gate' in name_lower:\n",
    "            return 'mlp_gate'\n",
    "        elif 'up' in name_lower:\n",
    "            return 'mlp_up'\n",
    "        elif 'down' in name_lower:\n",
    "            return 'mlp_down'\n",
    "        else:\n",
    "            return 'mlp_other'\n",
    "    elif 'norm' in name_lower or 'layer_norm' in name_lower:\n",
    "        return 'normalization'\n",
    "    elif 'lm_head' in name_lower or 'output' in name_lower:\n",
    "        return 'output_head'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def print_layers_by_type(layer_info: List[Dict]):\n",
    "    \"\"\"Print layers grouped by type for better organization.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    layers_by_type = defaultdict(list)\n",
    "    for layer in layer_info:\n",
    "        layers_by_type[layer['type']].append(layer)\n",
    "    \n",
    "    for layer_type in sorted(layers_by_type.keys()):\n",
    "        print(f\"\\n--- {layer_type.upper().replace('_', ' ')} LAYERS ---\")\n",
    "        for layer in layers_by_type[layer_type]:\n",
    "            print(f\"  {layer['name']:<50} | {str(layer['weight_shape']):<20} | {layer['module_type']}\")\n",
    "\n",
    "def print_gcg_summary(layer_info: List[Dict], config):\n",
    "    \"\"\"Print summary information useful for GCG implementation.\"\"\"\n",
    "    print(f\"\\n=== GCG Implementation Summary ===\")\n",
    "    \n",
    "    # Find transformer layers\n",
    "    transformer_layers = [l for l in layer_info if 'layers.' in l['name'] and l['type'] in ['attention_output', 'mlp_down']]\n",
    "    \n",
    "    if transformer_layers:\n",
    "        print(f\"Transformer layers detected: {len(set(l['name'].split('.layers.')[1].split('.')[0] for l in transformer_layers))}\")\n",
    "        \n",
    "        # Get representative dimensions\n",
    "        hidden_dim = config.hidden_size\n",
    "        print(f\"Hidden dimension: {hidden_dim}\")\n",
    "        \n",
    "        # Common target layers for refusal direction analysis\n",
    "        print(f\"\\nRecommended layers for refusal direction extraction:\")\n",
    "        print(f\"  - Residual stream dimension: {hidden_dim}\")\n",
    "        print(f\"  - MLP output layers: Look for 'mlp.down_proj' or similar\")\n",
    "        print(f\"  - Attention output layers: Look for 'self_attn.o_proj' or similar\")\n",
    "        \n",
    "        # Print some example layer names for targeting\n",
    "        example_layers = [l['name'] for l in transformer_layers[:5]]\n",
    "        print(f\"\\nExample targetable layer names:\")\n",
    "        for layer_name in example_layers:\n",
    "            print(f\"  - {layer_name}\")\n",
    "\n",
    "def analyze_specific_layers(model: nn.Module, layer_patterns: List[str] = None):\n",
    "    \"\"\"\n",
    "    Analyze specific layers matching given patterns.\n",
    "    Useful for targeting specific components in GCG.\n",
    "    \"\"\"\n",
    "    if layer_patterns is None:\n",
    "        # Default patterns for common refusal direction extraction points\n",
    "        layer_patterns = [\n",
    "            'layers.*.self_attn.o_proj',\n",
    "            'layers.*.mlp.down_proj',\n",
    "            'layers.*.mlp.gate_proj',\n",
    "            'model.layers.*.self_attn.o_proj',\n",
    "            'model.layers.*.mlp.down_proj'\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\n=== Analyzing Specific Layer Patterns ===\")\n",
    "    \n",
    "    import re\n",
    "    for pattern in layer_patterns:\n",
    "        print(f\"\\nPattern: {pattern}\")\n",
    "        regex_pattern = pattern.replace('*', r'\\d+')\n",
    "        \n",
    "        matching_layers = []\n",
    "        for name, module in model.named_modules():\n",
    "            if re.search(regex_pattern, name) and hasattr(module, 'weight'):\n",
    "                matching_layers.append((name, module.weight.shape))\n",
    "        \n",
    "        if matching_layers:\n",
    "            print(f\"  Found {len(matching_layers)} matching layers:\")\n",
    "            for name, shape in matching_layers[:3]:  # Show first 3\n",
    "                print(f\"    {name}: {shape}\")\n",
    "            if len(matching_layers) > 3:\n",
    "                print(f\"    ... and {len(matching_layers) - 3} more\")\n",
    "        else:\n",
    "            print(\"  No matching layers found\")\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage for analyzing DeepSeek R1 model dimensions.\n",
    "    Adapt the model loading code for your specific setup.\n",
    "    \"\"\"\n",
    "    # Replace with your actual model loading code\n",
    "    print(\"Loading DeepSeek R1 model...\")\n",
    "    \n",
    "    # Example model loading (adapt to your setup)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda:3\"\n",
    "    )\n",
    "    \n",
    "    # For demonstration, assuming you have a loaded model\n",
    "    # print_activation_dimensions(model, \"DeepSeek R1\")\n",
    "    # analyze_specific_layers(model)\n",
    "    \n",
    "    print(\"Replace the model loading section with your actual loaded DeepSeek R1 model.\")\n",
    "    print(\"Then call: print_activation_dimensions(your_model)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
