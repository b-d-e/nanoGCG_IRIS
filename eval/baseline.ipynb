{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05151ef6",
   "metadata": {},
   "source": [
    "## Get baseline refusal responses from DeepSeek on StrongREJECT Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b7715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from multiprocessing import Pool, Queue, Process, Manager\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "hf_cache = \"/scratch/etheridge/huggingface\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "DTYPE = \"bfloat16\"\n",
    "MAX_TOKENS = 2048\n",
    "SEED = 45\n",
    "\n",
    "# Multi-GPU Configuration\n",
    "GPUS = [1, 2, 3]  # Specify which GPUs to use\n",
    "PROCESSES_PER_GPU = 4 # Number of processes per GPU\n",
    "TOTAL_PROCESSES = len(GPUS) * PROCESSES_PER_GPU\n",
    "\n",
    "IN_CSV = \"../orthogonalized_outputs_2048.csv\"\n",
    "OUT_CSV = \"results/baseline_multi_gpu.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3262e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_on_device(gpu_id):\n",
    "    \"\"\"Initialize model on a specific GPU device\"\"\"\n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "    print(f\"Loading model on {device}...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=device,\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    return tokenizer, model, device\n",
    "\n",
    "def process_batch(args):\n",
    "    \"\"\"Process a batch of prompts on a specific GPU\"\"\"\n",
    "    batch_data, gpu_id, process_id = args\n",
    "    \n",
    "    try:\n",
    "        # Initialize model for this process\n",
    "        tokenizer, model, device = initialize_model_on_device(gpu_id)\n",
    "        \n",
    "        results = []\n",
    "        for idx, prompt in batch_data:\n",
    "            try:\n",
    "                # Tokenize input\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "                \n",
    "                # Generation parameters\n",
    "                generation_kwargs = {\n",
    "                    \"max_length\": MAX_TOKENS,\n",
    "                    \"do_sample\": False,\n",
    "                    \"num_beams\": 1,\n",
    "                    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "                }\n",
    "                \n",
    "                # Generate response\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(inputs[\"input_ids\"], **generation_kwargs)\n",
    "                \n",
    "                # Decode response\n",
    "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                results.append((idx, response))\n",
    "                \n",
    "                # Clear GPU memory\n",
    "                del inputs, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx} on GPU {gpu_id}: {str(e)}\")\n",
    "                results.append((idx, f\"ERROR: {str(e)}\"))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing GPU {gpu_id}, Process {process_id}: {str(e)}\")\n",
    "        return [(idx, f\"INIT_ERROR: {str(e)}\") for idx, _ in batch_data]\n",
    "\n",
    "def create_batches(df, num_processes):\n",
    "    \"\"\"Split dataframe into batches for parallel processing\"\"\"\n",
    "    indices = list(range(len(df)))\n",
    "    batch_size = len(indices) // num_processes\n",
    "    \n",
    "    batches = []\n",
    "    for i in range(num_processes):\n",
    "        start_idx = i * batch_size\n",
    "        if i == num_processes - 1:  # Last batch gets remaining items\n",
    "            end_idx = len(indices)\n",
    "        else:\n",
    "            end_idx = (i + 1) * batch_size\n",
    "        \n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        batch_data = [(idx, df.iloc[idx]['forbidden_prompt']) for idx in batch_indices]\n",
    "        \n",
    "        # Assign GPU based on process number\n",
    "        gpu_id = GPUS[i % len(GPUS)]\n",
    "        process_id = i // len(GPUS)\n",
    "        \n",
    "        batches.append((batch_data, gpu_id, process_id))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f51005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    print(\"Starting multi-GPU baseline evaluation...\")\n",
    "    print(f\"Using GPUs: {GPUS}\")\n",
    "    print(f\"Processes per GPU: {PROCESSES_PER_GPU}\")\n",
    "    print(f\"Total processes: {TOTAL_PROCESSES}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(IN_CSV)\n",
    "    df = df[[\"forbidden_prompt\"]].copy()\n",
    "    df[\"response\"] = None\n",
    "    df[\"row_idx\"] = df.index  # Add original row index\n",
    "    \n",
    "    print(f\"Dataset loaded: {len(df)} samples\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "    \n",
    "    # Create batches for parallel processing\n",
    "    batches = create_batches(df, TOTAL_PROCESSES)\n",
    "    print(f\"Created {len(batches)} batches\")\n",
    "    \n",
    "    # Process batches in parallel with progress bar\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=len(df), desc=\"Processing samples\", unit=\"samples\")\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=TOTAL_PROCESSES) as executor:\n",
    "        # Submit all batches\n",
    "        future_to_batch = {executor.submit(process_batch, batch): i \n",
    "                          for i, batch in enumerate(batches)}\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        all_results = []\n",
    "        completed_batches = 0\n",
    "        processed_samples = 0\n",
    "        \n",
    "        for future in as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            try:\n",
    "                batch_results = future.result()\n",
    "                all_results.extend(batch_results)\n",
    "                completed_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                batch_size = len(batch_results)\n",
    "                processed_samples += batch_size\n",
    "                pbar.update(batch_size)\n",
    "                pbar.set_postfix({\n",
    "                    'Batches': f\"{completed_batches}/{len(batches)}\",\n",
    "                    'GPUs': f\"{len(GPUS)}x{PROCESSES_PER_GPU}\",\n",
    "                    'Rate': f\"{processed_samples/(time.time()-start_time):.1f} samples/s\"\n",
    "                })\n",
    "                \n",
    "                # Save intermediate results\n",
    "                if completed_batches % 2 == 0 or completed_batches == len(batches):\n",
    "                    save_intermediate_results(df, all_results, processed_samples)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nBatch {batch_idx} generated an exception: {e}\")\n",
    "                pbar.update(len(batches[batch_idx][0]))  # Update progress even on error\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Update dataframe with results\n",
    "    print(\"Updating dataframe with results...\")\n",
    "    for idx, response in all_results:\n",
    "        df.at[idx, \"response\"] = response\n",
    "    \n",
    "    # Save final results\n",
    "    print(f\"Saving results to {OUT_CSV}...\")\n",
    "    df.to_csv(OUT_CSV, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    print(f\"Average time per sample: {total_time/len(df):.2f} seconds\")\n",
    "    print(f\"Processing rate: {len(df)/total_time:.1f} samples/second\")\n",
    "    print(f\"Samples processed: {len(df)}\")\n",
    "    print(f\"GPUs used: {len(GPUS)} (IDs: {GPUS})\")\n",
    "    print(f\"Processes per GPU: {PROCESSES_PER_GPU}\")\n",
    "    print(f\"Total processes: {TOTAL_PROCESSES}\")\n",
    "    print(f\"Results saved to: {OUT_CSV}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "def save_intermediate_results(df, results, processed_count):\n",
    "    \"\"\"Save intermediate results to prevent data loss\"\"\"\n",
    "    temp_df = df.copy()\n",
    "    for idx, response in results:\n",
    "        temp_df.at[idx, \"response\"] = response\n",
    "    \n",
    "    temp_file = OUT_CSV.replace('.csv', '_temp.csv')\n",
    "    temp_df.to_csv(temp_file, index=False)\n",
    "    \n",
    "    # Print status without disrupting progress bar\n",
    "    tqdm.write(f\"Checkpoint: {processed_count}/{len(df)} samples processed, saved to {temp_file}\")\n",
    "\n",
    "def monitor_gpu_usage():\n",
    "    \"\"\"Monitor GPU usage (optional utility function)\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                               '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if i in GPUS:\n",
    "                    util, mem_used, mem_total = line.split(', ')\n",
    "                    print(f\"GPU {i}: {util}% utilization, {mem_used}MB/{mem_total}MB memory\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not monitor GPU usage: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e088de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU state:\n",
      "GPU 1: 0% utilization, 18133MB/95830MB memory\n",
      "GPU 2: 0% utilization, 18651MB/95830MB memory\n",
      "GPU 3: 0% utilization, 18835MB/95830MB memory\n",
      "\n",
      "Starting multi-GPU baseline evaluation...\n",
      "Using GPUs: [1, 2, 3]\n",
      "Processes per GPU: 4\n",
      "Total processes: 12\n",
      "Loading dataset...\n",
      "Dataset loaded: 95 samples\n",
      "Created 12 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/95 [00:00<?, ?samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda:1...Loading model on cuda:2...Loading model on cuda:1...Loading model on cuda:3...Loading model on cuda:2...Loading model on cuda:3...\n",
      "\n",
      "\n",
      "Loading model on cuda:3...Loading model on cuda:1...Loading model on cuda:2...\n",
      "\n",
      "\n",
      "Loading model on cuda:3...Loading model on cuda:1...\n",
      "Loading model on cuda:2...\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0460fcae5a39416aae6de3ba1ad88ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0709e5994048beb7971068bb9fb625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534cd804dbc74ea6bdbf0cd1a3da81f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43df177fb234d0aa42d586721a3ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241a18fadbc14edfa204ebdec9999683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c774e776d8b46248165ba4b3f2ae0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5663620845024839bcfea3754ad7664a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1648f50fd844c508448c142f32420cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d3f1cec1b8467d9979e48f56b6f264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b8148923414a61a0c18d62cfaf3966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63f54c647e64c30ba03788f4eb794c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a0df3818ea432ba07fddd398e358e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/etheridge/nanoGCG_IRIS/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Processing samples:  15%|█▍        | 14/95 [05:22<25:56, 19.21s/samples, Batches=2/12, GPUs=3x4, Rate=0.0 samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 14/95 samples processed, saved to results/baseline_multi_gpu_temp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  29%|██▉       | 28/95 [05:37<07:44,  6.93s/samples, Batches=4/12, GPUs=3x4, Rate=0.1 samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 28/95 samples processed, saved to results/baseline_multi_gpu_temp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  44%|████▍     | 42/95 [05:53<03:03,  3.46s/samples, Batches=6/12, GPUs=3x4, Rate=0.1 samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 42/95 samples processed, saved to results/baseline_multi_gpu_temp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  59%|█████▉    | 56/95 [06:10<01:27,  2.25s/samples, Batches=8/12, GPUs=3x4, Rate=0.2 samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 56/95 samples processed, saved to results/baseline_multi_gpu_temp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:  74%|███████▎  | 70/95 [06:21<00:36,  1.47s/samples, Batches=10/12, GPUs=3x4, Rate=0.2 samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 70/95 samples processed, saved to results/baseline_multi_gpu_temp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 95/95 [09:13<00:00,  5.51s/samples, Batches=12/12, GPUs=3x4, Rate=0.2 samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 95/95 samples processed, saved to results/baseline_multi_gpu_temp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 95/95 [09:15<00:00,  5.84s/samples, Batches=12/12, GPUs=3x4, Rate=0.2 samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating dataframe with results...\n",
      "Saving results to results/baseline_multi_gpu.csv...\n",
      "\n",
      "============================================================\n",
      "BASELINE EVALUATION COMPLETE\n",
      "============================================================\n",
      "Total time: 555.25 seconds (9.3 minutes)\n",
      "Average time per sample: 5.84 seconds\n",
      "Processing rate: 0.2 samples/second\n",
      "Samples processed: 95\n",
      "GPUs used: 3 (IDs: [1, 2, 3])\n",
      "Processes per GPU: 4\n",
      "Total processes: 12\n",
      "Results saved to: results/baseline_multi_gpu.csv\n",
      "============================================================\n",
      "\n",
      "Final GPU state:\n",
      "GPU 1: 22% utilization, 18133MB/95830MB memory\n",
      "GPU 2: 17% utilization, 18651MB/95830MB memory\n",
      "GPU 3: 0% utilization, 18835MB/95830MB memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Optional: Monitor initial GPU state\n",
    "    print(\"Initial GPU state:\")\n",
    "    monitor_gpu_usage()\n",
    "    print()\n",
    "    \n",
    "    # Run main processing\n",
    "    main()\n",
    "    \n",
    "    # Optional: Monitor final GPU state\n",
    "    print(\"\\nFinal GPU state:\")\n",
    "    monitor_gpu_usage()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
